{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsz8CE2Qlta2"
   },
   "source": [
    "# ER / NR discrimination test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "XuVS7q57lta8",
    "outputId": "1375f3b0-bde1-4ddc-e539-3354230fda75"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/FlamTeam/flamedisx-notebooks/master/_if_on_colab_setup_flamedisx.ipynb\n",
    "BRANCH=\"master\"  # git branch to use (only for Colab)\n",
    "%run _if_on_colab_setup_flamedisx.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kAtsuykGltbI",
    "outputId": "36adffc6-076f-47be-d39c-ecb32939683e"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from multihist import Hist1d, Histdd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flamedisx as fd\n",
    "\n",
    "have_gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(f\"Running tensorflow {tf.__version__}, GPU {'active' if have_gpu else 'NOT ACTIVE!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDCTxQM-ltbP"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "kXuIGV-lhXYy",
    "outputId": "79186484-a859-478d-d1dc-6074d648351f"
   },
   "outputs": [],
   "source": [
    "# Construct toy spatial distribution of neutron background source\n",
    "\n",
    "tau = 5  # cm  attenuation\n",
    "nbins = 100\n",
    "\n",
    "# Full dimensions of TPC (not FV!)\n",
    "tpc_radius = 47.9  # cm\n",
    "tpc_length = 97.6  # cm\n",
    "\n",
    "# XENON1T 1.3t FV\n",
    "fv_radius = 41.26  # cm\n",
    "fv_high = -9  # cm\n",
    "fv_low = -92.9  # cm\n",
    "\n",
    "r = np.linspace(0, fv_radius, nbins + 1)\n",
    "z = np.linspace(fv_low, fv_high, nbins + 1)\n",
    "theta = np.linspace(0, 2 * np.pi, nbins + 1)\n",
    "\n",
    "# Create test background histogram\n",
    "spatial_h = Histdd(bins=[r, theta, z], axis_names=['r', 'theta', 'z'])\n",
    "\n",
    "r_centers, theta_centers, z_centers = spatial_h.bin_centers()\n",
    "\n",
    "# compute r,z function\n",
    "rr, zz = np.meshgrid(r_centers, z_centers)\n",
    "\n",
    "def rate(r, z):\n",
    "    # Rate at position r,z in wrt full TPC\n",
    "    r_edge = tpc_radius - r\n",
    "    z_edge_top = -z\n",
    "    z_edge_bot =  tpc_length + z\n",
    "\n",
    "    # Add a little z dependent tau to make dist more round\n",
    "    tau_edge = tau * (0.9 + ((2*z + tpc_length)/tpc_length)**2)\n",
    "\n",
    "    # Add constant component\n",
    "    c = 5e-3\n",
    "\n",
    "    return c + np.exp(-r_edge/tau_edge) + np.exp(-z_edge_top/tau) + np.exp(-z_edge_bot/tau)\n",
    "\n",
    "vv = rate(rr, zz)\n",
    "\n",
    "# Set each theta slice\n",
    "for idx in range(len(theta_centers)):\n",
    "    spatial_h.histogram[:,idx,:] = vv.T\n",
    "\n",
    "# Normalize\n",
    "spatial_h.histogram /= spatial_h.n\n",
    "# Compute bin volumes for cylindrical coords (r dr dtheta)\n",
    "bin_volumes = spatial_h.bin_volumes() * r_centers[:, np.newaxis, np.newaxis]\n",
    "spatial_h.histogram *= bin_volumes\n",
    "\n",
    "sample = spatial_h.get_random(1_000_000)\n",
    "\n",
    "test_h = Histdd(sample.T[0]**2, sample.T[2], bins=100, axis_names=['r2', 'z'])\n",
    "test_h.plot(log_scale=True, cmap='jet', log_scale_vmin=1e-1)\n",
    "plt.xlim(0, tpc_radius**2)\n",
    "plt.ylim(-tpc_length, 0)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRir9cNqXWW5"
   },
   "outputs": [],
   "source": [
    "# Load energy spectrum for neutron background, use up to 50 keV\n",
    "import pandas as pd\n",
    "radiogenics = pd.read_csv(\"radiogenic_spectrum.csv\", names=['energy', 'rate'])\n",
    "radiogenics = radiogenics[radiogenics['energy'] < 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Msxgll3DltbR"
   },
   "outputs": [],
   "source": [
    "elife=326e3\n",
    "\n",
    "class LowMassWIMPSource(fd.x1t_sr0.SR0WIMPSource):\n",
    "    fv_radius = fv_radius\n",
    "    fv_high = fv_high\n",
    "    fv_low = fv_low\n",
    "    mw = 200  # GeV\n",
    "    pretend_wimps_dont_modulate = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def electron_detection_eff(drift_time, *, elife=elife, extraction_eff=0.96):\n",
    "        return extraction_eff * tf.exp(-drift_time / elife)\n",
    "\n",
    "class LowMassWIMPModulationSource(LowMassWIMPSource):\n",
    "    pretend_wimps_dont_modulate = False\n",
    "\n",
    "class LowEnergyERSource(fd.x1t_sr0.SR0ERSource):\n",
    "    fv_radius = fv_radius\n",
    "    fv_high = fv_high\n",
    "    fv_low = fv_low\n",
    "    def _single_spectrum(self):\n",
    "        \"\"\"Return (energies in keV, rate at these energies),\n",
    "        \"\"\"\n",
    "        return (tf.dtypes.cast(\n",
    "                    tf.linspace(0., 10., 1000),  # 10 keV for 1 TeV WIMP\n",
    "                    dtype=fd.float_type()),\n",
    "                tf.ones(1000, dtype=fd.float_type()))\n",
    "    \n",
    "    @staticmethod\n",
    "    def electron_detection_eff(drift_time, *, elife=elife, extraction_eff=0.96):\n",
    "        return extraction_eff * tf.exp(-drift_time / elife)\n",
    "\n",
    "#class NRSpectrum(fd.x1t_sr0.SR0WIMPSource):\n",
    "#    # Make a time-averaged 200 GeV WIMP spectrum\n",
    "#    mw = 10  # GeV\n",
    "#    pretend_wimps_dont_modulate = True\n",
    "#\n",
    "#nr_spectrum_hist = NRSpectrum().energy_hist\n",
    "\n",
    "class NRBackgroundSource(fd.x1t_sr0.SR0NRSource):\n",
    "    # The NR background is not uniform in space,\n",
    "    # use spatial_rate_hist to model this\n",
    "    spatial_rate_hist = spatial_h\n",
    "    spatial_rate_bin_volumes = bin_volumes\n",
    "    # neutron energy spectrum\n",
    "    def _single_spectrum(self):\n",
    "        \"\"\"Return (energies in keV, rate at these energies),\n",
    "        \"\"\"\n",
    "        #return (tf.dtypes.cast(nr_spectrum_hist.bin_centers()[1],\n",
    "        #                       dtype=fd.float_type()),\n",
    "        #        tf.dtypes.cast(nr_spectrum_hist.histogram[0],\n",
    "        #                       dtype=fd.float_type()))\n",
    "        return (tf.dtypes.cast(radiogenics['energy'],\n",
    "                               dtype=fd.float_type()),\n",
    "                tf.dtypes.cast(radiogenics['rate'] * 1000 * 365.25, # ev/tonne/year\n",
    "                               dtype=fd.float_type()))\n",
    "    \n",
    "    @staticmethod\n",
    "    def electron_detection_eff(drift_time, *, elife=elife, extraction_eff=0.96):\n",
    "        return extraction_eff * tf.exp(-drift_time / elife)\n",
    "\n",
    "\n",
    "def add_corrected_signals(d):\n",
    "    d['cs1'] = (0.142 / (1 + 0.219)) * d['s1'] / (\n",
    "        d['photon_detection_eff'] * d['photon_gain_mean'])\n",
    "    d['cs2'] = (11.4 / (1 - 0.63) / 0.96) * d['s2'] / (\n",
    "        d['electron_detection_eff'] * d['electron_gain_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "1yzT0VDnwQNG",
    "outputId": "36577222-b5e3-4b94-8ea4-25042425aa9a"
   },
   "outputs": [],
   "source": [
    "dsets = dict(\n",
    "    er=dict(source_class=LowEnergyERSource),\n",
    "    nr=dict(source_class=LowMassWIMPSource),\n",
    "    nr_bkg=dict(source_class=NRBackgroundSource),\n",
    "    nr_mod=dict(source_class=LowMassWIMPModulationSource))\n",
    "\n",
    "for k, v in dsets.items():\n",
    "    dsets[k]['source'] = v['source_class'](\n",
    "        batch_size=300 if have_gpu else 10, \n",
    "        max_sigma=5)\n",
    "    \n",
    "nr_conditions = ['nr', 'nr_mod']\n",
    "\n",
    "for nrc in nr_conditions:\n",
    "    dsets[nrc]['source'].energy_hist.sum(axis=1).plot(label=nrc)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOiYkhtHqqdW"
   },
   "source": [
    "## Compute rate histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Xc1L9n2Ultbk",
    "outputId": "fe8c054b-9695-4cfe-a8f9-3c5f255856f7"
   },
   "outputs": [],
   "source": [
    "def std_axes():\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"cS1 [PE]\")\n",
    "    plt.ylabel(\"cS2 [PE]\")\n",
    "\n",
    "\n",
    "# Restore histograms\n",
    "#fn = 'hists_1e6_20200213_30GeV_326elife.pkl'\n",
    "#fn = 'hists_1e6_20200218_15GeV_326elife.pkl'\n",
    "fn = 'hists_1e6_20200217_200GeV_326elife.pkl'\n",
    "with open(fn, mode='rb') as f:\n",
    "    hists = pickle.load(f)\n",
    "    for dname, q in dsets.items():\n",
    "        if dname not in hists:\n",
    "            continue\n",
    "        q['mh'] = hists[dname]\n",
    "\n",
    "remake_hists = False\n",
    "\n",
    "for dname, q in dsets.items():\n",
    "\n",
    "    if remake_hists or not 'mh' in q:\n",
    "        print(f\"Building histogram for {dname}\")\n",
    "        \n",
    "        mh = Histdd(bins=(\n",
    "            np.linspace(0, 80, 81 + 1),\n",
    "            np.geomspace(10**1.7 / (1 - 0.63),\n",
    "                         10**3.9 / (1 - 0.63), \n",
    "                         70)))\n",
    "        \n",
    "        n_batches = 100 if dname == 'er' else 40\n",
    "        trials_per_batch = int(1e6)\n",
    "\n",
    "        for _ in tqdm(range(n_batches)):\n",
    "            d = q['source'].simulate(trials_per_batch)\n",
    "            add_corrected_signals(d)\n",
    "            mh.add(d['cs1'], d['cs2'])\n",
    "        \n",
    "        # Convert to PDF\n",
    "        mh /= mh.bin_volumes() * trials_per_batch * n_batches\n",
    "\n",
    "        # Multiply by total expected event rate\n",
    "        # (from the source, i.e. before correcting for efficiencies)\n",
    "        mh *= q['source'].mu_before_efficiencies()\n",
    "\n",
    "        q['mh'] = mh\n",
    "\n",
    "    q['events_per_bin'] = q['mh'] * q['mh'].bin_volumes()\n",
    "    q['mh'].plot(cblabel='rate * PDF')\n",
    "    plt.title(f\"{dname}: {q['events_per_bin'].n:.02f} expected events\")\n",
    "    std_axes()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vL5V5W6J-poe"
   },
   "outputs": [],
   "source": [
    "# # If you save histograms, remember to download them!!\n",
    "# with open('hists_1e6_20200205_15GeV.pkl', mode='wb') as f:\n",
    "#     pickle.dump({\n",
    "#         k: v['mh'] \n",
    "#         for k, v in dsets.items()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VG88gt7ltbq"
   },
   "source": [
    "  * Make sure none of the models are 'cut off' in cS1 / cS2, since cS1 and cS2 cut acceptances are not currently accounted for in our likelihood (unlike S1 or S2 cuts). This is not a limitation of flamedisx: the correction value is known for each event since the correction depends only on observables, so ultimately a cS1 cut is just a space-dependent S1 cut (which flamedisx fully supports).\n",
    "  * The ROC curves will depend on the extent of the ER spectrum. If you include more high-energy ER events that can be discriminated anyway, the ER leakage in any likelihood will go down. The key figure of merit we are trying to derive here, the decrease in ER leakage at ~50 % NR acceptance when switching to the full likelihood, should be unaffected by this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1xvN-dZltbs"
   },
   "source": [
    "## Histogram-based discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "zIRb00atjD49",
    "outputId": "8bbba2e7-06b6-470b-f094-2722b3bcd1d4"
   },
   "outputs": [],
   "source": [
    "for dname in nr_conditions:\n",
    "    q = dsets[dname]\n",
    "    q['signal_background_ratio'] = ratio = \\\n",
    "        (dsets['er']['mh'] + 1e-20)/(q['mh'] + 1e-20)\n",
    "    q['histogram_ordering'] = np.argsort(ratio.histogram.ravel())\n",
    "\n",
    "    ratio.plot(log_scale=True, cmap=plt.cm.seismic, \n",
    "           vmin=1e-4, vmax=1e4, cblabel='ER / NR density ratio')\n",
    "    std_axes()\n",
    "    plt.title(f\"er vs {dname}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "vVjWl8DsnEMi",
    "outputId": "81e466ca-7531-46be-e6c6-4289d76b4fbb"
   },
   "outputs": [],
   "source": [
    "# For NR bkg\n",
    "for dname in nr_conditions:\n",
    "    q = dsets[dname]\n",
    "    q['signal_background_ratio_nrbkg'] = ratio = \\\n",
    "        (dsets['nr_bkg']['mh'] + 1e-20)/(q['mh'] + 1e-20)\n",
    "    q['histogram_ordering_nrbkg'] = np.argsort(ratio.histogram.ravel())\n",
    "\n",
    "    ratio.plot(log_scale=True, cmap=plt.cm.seismic, \n",
    "           vmin=1e-4, vmax=1e4, cblabel='NRBKG / NR density ratio')\n",
    "    std_axes()\n",
    "    plt.title(f\"nrbkg vs {dname}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NR WIMP vs Modulating WIMP\n",
    "for dname in nr_conditions:\n",
    "    q = dsets[dname]\n",
    "    q['signal_background_ratio_mod'] = ratio = \\\n",
    "        (dsets['nr']['mh'] + 1e-20)/(q['mh'] + 1e-20)\n",
    "    q['histogram_ordering_mod'] = np.argsort(ratio.histogram.ravel())\n",
    "\n",
    "    ratio.plot(log_scale=True, cmap=plt.cm.seismic, \n",
    "           vmin=1e-4, vmax=1e4, cblabel='non-mod / mod WIMP density ratio')\n",
    "    std_axes()\n",
    "    plt.title(f\"non-mod vs {dname}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "m6cvL0XaxrHA",
    "outputId": "ade4ae9b-acb1-45c2-ffad-95165f6cd0d6"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.gcf().patch.set_facecolor('white')\n",
    "\n",
    "def hist_to_cdf(hist, ordering):\n",
    "    return np.cumsum(hist.histogram.ravel()[ordering])/hist.n\n",
    "\n",
    "for dname in nr_conditions:\n",
    "    q = dsets[dname]\n",
    "    q['roc_from_histogram'] = (\n",
    "        hist_to_cdf(dsets['er']['events_per_bin'], q['histogram_ordering']),\n",
    "        hist_to_cdf(q['events_per_bin'], q['histogram_ordering']))\n",
    "\n",
    "    plt.plot(*q['roc_from_histogram'], label='er vs. ' + dname)\n",
    "\n",
    "    q['roc_from_histogram_nrbkg'] = (\n",
    "        hist_to_cdf(dsets['nr_bkg']['events_per_bin'], q['histogram_ordering_nrbkg']),\n",
    "        hist_to_cdf(q['events_per_bin'], q['histogram_ordering_nrbkg']))\n",
    "\n",
    "    plt.plot(*q['roc_from_histogram_nrbkg'], label='nr bkg vs. ' + dname)\n",
    "    \n",
    "    q['roc_from_histogram_mod'] = (\n",
    "        hist_to_cdf(dsets['nr']['events_per_bin'], q['histogram_ordering_mod']),\n",
    "        hist_to_cdf(q['events_per_bin'], q['histogram_ordering_mod']))\n",
    "\n",
    "    plt.plot(*q['roc_from_histogram_mod'], label='non-mod vs. ' + dname)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.axhline(0.5, alpha=0.5, c='k', linewidth=0.5)\n",
    "\n",
    "plt.xlim(1e-4, 1)\n",
    "plt.xlabel(\"ER rejection\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"NR acceptance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBTVqZtWzBG1"
   },
   "source": [
    "As expected, there is no difference between NR with and without modulation, since the 2D histogram does not see time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cc6yHQr3zFGY"
   },
   "source": [
    "## Flamedisx-based discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "wXInc439H0d3",
    "outputId": "5093b058-eb19-402c-bcef-07277b0186e7"
   },
   "outputs": [],
   "source": [
    "# Load earlier results: \n",
    "#fn = 'discstudy_20200218_1e6_maxsigma5_15GeV_326elife.pkl.gz'\n",
    "#fn = 'discstudy_20200213_1e6_maxsigma5_30GeV_326elife.pkl.gz'\n",
    "fn = 'discstudy_20200217_1e6_maxsigma5_200GeV_326elife.pkl.gz'\n",
    "with gzip.open(fn, mode='rb') as f:\n",
    "    q = pickle.load(f)\n",
    "    for k, v in q.items():\n",
    "        dsets[k]['data'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HtdsmoQwQN2"
   },
   "outputs": [],
   "source": [
    "remake_data = False\n",
    "n_trials_events = int(1e6) # if have_gpu else int(1e4)\n",
    "\n",
    "for dname, q in dsets.items():\n",
    "    # Get simulated data\n",
    "    if remake_data or 'data' not in q:\n",
    "        print(f\"Simulating data for {dname}\")\n",
    "        q['data'] = sim_data = q['source'].simulate(n_trials_events)\n",
    "        sim_data['event_time'] += int(5e9)  # Add 5 seconds\n",
    "        add_corrected_signals(sim_data)\n",
    "        \n",
    "        # Ensure cs1 and cs2 are in range of the histogram\n",
    "        # to avoid extrapolation in multihist's lookup.\n",
    "        # NB: we are assuming both histograms have the same binning here!\n",
    "        bes = q['mh'].bin_edges\n",
    "        mask = (\n",
    "            (bes[0][0] <= sim_data['cs1']) & (sim_data['cs1'] < bes[0][-1]) &\n",
    "            (bes[1][0] <= sim_data['cs2']) & (sim_data['cs2'] < bes[1][-1]))\n",
    "        print(f\"{dname}: Throwing out {100 * (~mask).sum() / len(sim_data):.2f}% of events\")\n",
    "        q['data'] = sim_data[mask].copy()\n",
    "    sim_data = q['data']\n",
    "    \n",
    "    # Compute differential rates (\"likelihoods\") for all models (\"sources\")\n",
    "    for likelihood_dsetname, likelihood_dset in dsets.items():\n",
    "        sim_data['l_mh_' + likelihood_dsetname] = likelihood_dset['mh'].lookup(\n",
    "            sim_data['cs1'], sim_data['cs2'])\n",
    "\n",
    "        key = 'l_full_' + likelihood_dsetname\n",
    "        if key not in sim_data:\n",
    "            print(f\"Computing likelihood of {dname} data under {likelihood_dsetname} model\")\n",
    "            likelihood_dset['source'].set_data(sim_data.copy())\n",
    "            sim_data[key] = likelihood_dset['source'].batched_differential_rate()\n",
    "        \n",
    "    # Compute ratios of differential rates (\"likelihood ratios\")\n",
    "    # for both histogram- and flamedisx based method.\n",
    "    for nrc in nr_conditions:\n",
    "        sim_data[f'lr_mh_{nrc}'] = sim_data['l_mh_er'] / sim_data[f'l_mh_{nrc}']\n",
    "        sim_data[f'lr_full_{nrc}'] = sim_data['l_full_er'] / sim_data[f'l_full_{nrc}']\n",
    "\n",
    "        sim_data[f'lr_mh_nr_{nrc}'] = sim_data['l_mh_nr_bkg'] / sim_data[f'l_mh_{nrc}']\n",
    "        sim_data[f'lr_full_nr_{nrc}'] = sim_data['l_full_nr_bkg'] / sim_data[f'l_full_{nrc}']\n",
    "\n",
    "    sim_data[f'lr_mh_mod'] = sim_data['l_mh_nr'] / sim_data[f'l_mh_nr_mod']\n",
    "    sim_data[f'lr_full_mod'] = sim_data['l_full_nr'] / sim_data[f'l_full_nr_mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3pqYqc7yUMJ"
   },
   "outputs": [],
   "source": [
    "# Save results to gzipped pickle. Compression takes a while; download longer.\n",
    "# Don't forget to download it!!\n",
    "#with gzip.open('discstudy_20200205_1e6_maxsigma5_15GeV.pkl.gz', mode='wb') as f:\n",
    "#    pickle.dump({dname: q['data']\n",
    "#                 for dname, q in dsets.items()},\n",
    "#                f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ubzxw4Cjltcf"
   },
   "source": [
    "## Compare differential rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NcOSJiWltcg"
   },
   "source": [
    "Compare differential rates. There will be an offset because (cS1, cS2) and (S1, S2) have different ranges/means -- so the rates are differential with respect to different coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "4iJqI_ff3ZU7",
    "outputId": "14150b53-bbde-4231-9ea0-b6eb1a395c58"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for dn_i, dname in enumerate(['er', 'nr']):\n",
    "    for lh_i, lh_name in enumerate(['er', 'nr']):\n",
    "        ax = axes[dn_i, lh_i]\n",
    "        plt.sca(ax)\n",
    "        \n",
    "        q = dsets[dname]['data']\n",
    "        if dname == 'er' and lh_name == 'er':\n",
    "            q_s = q.copy()\n",
    "        y, x = q[f'l_full_{lh_name}'], q[f'l_mh_{lh_name}']\n",
    "\n",
    "        Histdd(x, y,\n",
    "               bins=(np.geomspace(1e-7, 1e-1, 100),\n",
    "                     np.geomspace(1e-7, 1e-1, 100))).plot(\n",
    "            log_scale=True, cblabel='Events/bin',\n",
    "            vmin=0.8, vmax=n_trials_events * 0.05)\n",
    "        \n",
    "        #xs = np.geomspace(1e-7, 1e-1, 100)\n",
    "        #plt.plot(xs, xs * 1e2, 'r')\n",
    "        #plt.plot(xs, xs * 1e1, 'r')\n",
    "        #plt.plot(xs, xs * 2.8, 'r')\n",
    "\n",
    "        plt.plot([1e-7, 1e-1], [1e-7, 1e-1], 'k-')\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Histogram\")\n",
    "        plt.ylabel(\"Flamedisx\")\n",
    "        plt.ylim(1e-7, 1e-1)\n",
    "        plt.xlim(1e-7, 1e-1)\n",
    "        plt.title(f\"{dname.upper()} data, {lh_name.upper()} model\")\n",
    "        plt.gca().set_aspect(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "_Zh1j7N2kJp-",
    "outputId": "b9ef3b3b-0aef-4152-f749-8b480e0aede4"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "min_ll = 1e-7\n",
    "for dn_i, dname in enumerate(['nr_bkg', 'nr']):\n",
    "    for lh_i, lh_name in enumerate(['nr_bkg', 'nr']):\n",
    "        ax = axes[dn_i, lh_i]\n",
    "        plt.sca(ax)\n",
    "        \n",
    "        q = dsets[dname]['data']\n",
    "        y, x = q[f'l_full_{lh_name}'], q[f'l_mh_{lh_name}']\n",
    "\n",
    "        Histdd(x, y,\n",
    "               bins=(np.geomspace(min_ll, 1e-1, 100),\n",
    "                     np.geomspace(min_ll, 1e-1, 100))).plot(\n",
    "            log_scale=True, cblabel='Events/bin',\n",
    "            vmin=0.8, vmax=n_trials_events * 0.05)\n",
    "\n",
    "        plt.plot([min_ll, 1e-1], [min_ll, 1e-1], 'k-')\n",
    "        plt.yscale('log')\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Histogram\")\n",
    "        plt.ylabel(\"Flamedisx\")\n",
    "        plt.ylim(min_ll, 1e-1)\n",
    "        plt.xlim(min_ll, 1e-1)\n",
    "        plt.title(f\"{dname.upper()} data, {lh_name.upper()} model\")\n",
    "        plt.gca().set_aspect(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99KiYcYJltcn"
   },
   "outputs": [],
   "source": [
    "# # Zoom-in on the low-energy NR data\n",
    "# d = dsets['nr']['data']\n",
    "# dsets['er']['mh'].plot(log_scale=True, vmin=1e-6, vmax=1e-1, cmap=plt.cm.Blues, cblabel=\"Diffrate hist\")\n",
    "# plt.scatter(d['cs1'], d['cs2'], c=d['l_full_er'], s=0.1, \n",
    "#             vmax=1e-1, vmin=1e-6, norm=matplotlib.colors.LogNorm(), cmap=plt.cm.Reds)\n",
    "# plt.colorbar(label='Diffrate Flamedisx')\n",
    "# plt.xlim(0, 10)\n",
    "# plt.xlabel(\"cS1 [PE]\")\n",
    "# plt.ylim(0, 3e3)\n",
    "# plt.ylabel(\"cS2 [PE]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZFDXyao7rycZ",
    "outputId": "eb7a7224-8709-4c40-f4d2-abb116e38df2"
   },
   "outputs": [],
   "source": [
    "# Check spatial distribution of sources\n",
    "for dname in ['er', 'nr', 'nr_bkg', 'nr_mod']:\n",
    "    q = dsets[dname]['data']\n",
    "    print(len(q))\n",
    "    Histdd(q['r']**2, q['z'], bins=100, axis_names=['r2', 'z']).plot(log_scale=True)\n",
    "    plt.title(dname)\n",
    "    plt.xlim(0, tpc_radius**2)\n",
    "    plt.ylim(-tpc_length, 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGNYu8vvltct"
   },
   "source": [
    "## Compare event-by-event discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyaxFw_4ltcu"
   },
   "outputs": [],
   "source": [
    "# d_er = dsets['er']['data']\n",
    "# d_nr = dsets['nr']['data']\n",
    "\n",
    "# for d, alt, cmap in [#(d_er, d_nr, plt.cm.viridis),\n",
    "#                      (d_nr, d_er, plt.cm.magma)\n",
    "#                     ]:\n",
    "#     # For each event in d, find what fraction of the alt data is more NR-like than it\n",
    "#     # (under both likelihoods)\n",
    "#     f_above = {\n",
    "#         lt: np.searchsorted(np.sort(alt[f'lr_{lt}_nr'].values), \n",
    "#                             d[f'lr_{lt}_nr'].values).astype(np.float) / len(alt)\n",
    "#         for lt in ('mh', 'full')}\n",
    "\n",
    "#     # Get ratio. \n",
    "#     #   0 = mh sees the event as more NR-like than any of the alt data\n",
    "#     #   > 1: mh is worse at discriminating, < 1 mh is better at discriminating\n",
    "#     ratio = f_above['mh'] / f_above['full']\n",
    "#     mask = np.isfinite(ratio)\n",
    "    \n",
    "#     xkey, ykey = 's1', 'z'\n",
    "    \n",
    "#     plt.scatter(d[xkey][mask], d[ykey][mask], c=ratio[mask], #cmap=cmap,\n",
    "#                 vmin=0, vmax=2, cmap=plt.cm.seismic,\n",
    "#                 s=0.2,)\n",
    "#     plt.colorbar()\n",
    "#     plt.scatter(d[xkey][~mask], d[ykey][~mask], c='g',\n",
    "#                 s=0.2, alpha=0.2)\n",
    "    \n",
    "#     plt.xlabel(xkey)\n",
    "#     #plt.ylim(0, 700)\n",
    "#     plt.ylabel(ykey)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEcKKegvltcz"
   },
   "source": [
    "For these NR events, red events have a more NR-like ER/NR likelihood ratio in flamedisx, and blue ones in the histogram likelihood. For green events the ratio of the two likelihood ratios is not a finite number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEA9K_-Bltc5"
   },
   "source": [
    "Log likelihood ratio histograms for ER and NR data under both likelihoods below. Note many events are at the edges for the histogram likelihood; for these either the NR or ER histogram was zero, and we clip the likelihood ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fVSlcJNzOYRU",
    "outputId": "556e63e1-5f1c-40ca-b47c-35bd7d1dfa24"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.gcf().patch.set_facecolor('white')\n",
    "for lt, nrc, color in [['mh', 'nr', 'b'], ['full', 'nr', 'g'], ['full', 'nr_mod', 'purple']]:\n",
    "    hists = dict()\n",
    "    cis = dict()\n",
    "    for dname, q in dsets.items():\n",
    "        if dname not in ('er', nrc):\n",
    "            continue\n",
    "\n",
    "        d = q['data'][f'lr_{lt}_{nrc}']\n",
    "        print(f\"Found {np.sum(np.isnan(d))} NaNs for {lt}:{dname}\")\n",
    "        clip_exp = 21\n",
    "        hists[dname] = Hist1d(\n",
    "            np.log10(d.clip(10**-clip_exp, 10**clip_exp).values.astype('float')),\n",
    "            bins=np.linspace(-clip_exp, clip_exp + 0.1, 140))\n",
    "        hists[dname].plot(label=f\"{dname}{lt}: {hists[dname].n}\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "#plt.axvline(-12, color='k', linestyle='--')\n",
    "#plt.axvline(-11, color='k', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxU8HT6IltdC"
   },
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHGJKjNGnrpf"
   },
   "outputs": [],
   "source": [
    "def binom_interval(success, total, conf_level=0.95):\n",
    "    \"\"\"Confidence interval on binomial - using Jeffreys interval\n",
    "    Code stolen from https://gist.github.com/paulgb/6627336\n",
    "    Agrees with http://statpages.info/confint.html for binom_interval(1, 10)\n",
    "    \"\"\"\n",
    "    # TODO: special case for success = 0 or = total? see wikipedia\n",
    "    quantile = (1 - conf_level) / 2.\n",
    "    lower = stats.beta.ppf(quantile, success, total - success + 1)\n",
    "    upper = stats.beta.ppf(1 - quantile, success + 1, total - success)\n",
    "    \n",
    "    # If something went wrong with a limit calculation, report the trivial limit\n",
    "    lower[np.isnan(lower)] = 0\n",
    "    upper[np.isnan(upper)] = 1\n",
    "    return lower, upper\n",
    "\n",
    "def make_roc(key, dnames, nbins=10_000):\n",
    "    hists = dict()\n",
    "    cdfs = dict()\n",
    "    cdf_intervals = dict()\n",
    "    for dname in dnames:\n",
    "        q = dsets[dname]\n",
    "        hists[dname] = Hist1d(\n",
    "            np.log10(q['data'][key].clip(1e-20, 1e20)),\n",
    "            bins=np.linspace(-21, 21, nbins))\n",
    "\n",
    "        cdfs[dname] = hists[dname].cumulative_density\n",
    "        cdf_intervals[dname] = np.stack(binom_interval(\n",
    "            np.cumsum(hists[dname].histogram),\n",
    "            hists[dname].n, \n",
    "            conf_level=.68))\n",
    "\n",
    "    return dict(hists=hists, cdfs=cdfs, cdf_intervals=cdf_intervals)\n",
    "\n",
    "# 'regular (wrt ER)': lr_mh_nr, lr_full_nr, lr_full_nr_mod\n",
    "# dnames 'er' 'nr'\n",
    "rocs = dict()\n",
    "for lt, nrc in [['mh', 'nr'], ['full', 'nr'], ['full', 'nr_mod']]:\n",
    "    key = f'lr_{lt}_{nrc}'\n",
    "    rocs[(lt, nrc)] = make_roc(key, dnames=['er', 'nr'])\n",
    "\n",
    "# 'wrt nr bkg': lr_mh_nr, lr_full_nr, lr_full_nr_mod\n",
    "# dnames 'nr_bkg' 'nr'\n",
    "rocs_nrbkg = dict()\n",
    "for lt, nrc in [['mh', 'nr'], ['full', 'nr'], ['full', 'nr_mod']]:\n",
    "    key = f'lr_{lt}_nr_{nrc}'\n",
    "    rocs_nrbkg[(lt, nrc)] = make_roc(key, dnames=['nr_bkg', 'nr'])\n",
    "\n",
    "# 'wrt fd nr': lr_full_nr, lr_full_nr_mod\n",
    "# dnames 'nr' 'nr_mod'\n",
    "rocs_mod = dict()\n",
    "for lt, nrc in [['mh', 'mod'], ['full', 'mod']]:\n",
    "    key = f'lr_{lt}_{nrc}'\n",
    "    rocs_mod[(lt, nrc)] = make_roc(key, dnames=['nr', 'nr_mod'], nbins=100_000)\n",
    "\n",
    "roc_labels = {('mh', 'nr'): \"2d Hist (sampled)\",\n",
    "              ('full', 'nr'): \"Flamedisx\",\n",
    "              ('full', 'nr_mod'): \"Fd modulation\",\n",
    "              ('mh', 'mod'): \"2d Hist (sampled)\",\n",
    "              ('full', 'mod'): \"Fd modulation\",\n",
    "             }\n",
    "roc_colors = {('mh', 'nr'): 'b',\n",
    "              ('full', 'nr'): 'g',\n",
    "              ('full', 'nr_mod'): 'darkorange',\n",
    "              ('mh', 'mod'): 'b',\n",
    "              ('full', 'mod'): 'darkorange',\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "colab_type": "code",
    "id": "79fwHbSs5oxX",
    "outputId": "9a67ef92-d454-46fa-97ca-c14e0181bce6"
   },
   "outputs": [],
   "source": [
    "# for rn, roc in rocs.items():\n",
    "#     x, y = roc['cdfs']['er'], roc['cdfs']['nr']\n",
    "#     plt.errorbar(x, y,\n",
    "#                  xerr=np.abs(roc['cdf_intervals']['er'] - x),\n",
    "#                  yerr=np.abs(roc['cdf_intervals']['nr'] - y),\n",
    "#                  linestyle='',\n",
    "#                  color=roc_colors[rn],\n",
    "#                  label=roc_labels[rn])\n",
    "# plt.plot(*dsets['nr']['roc_from_histogram'], \n",
    "#          label='2D Hist ($N = \\infty$)',\n",
    "#          color='k', linestyle='--')\n",
    "#      \n",
    "# plt.xlabel(\"ER (flat 0-10 keV) background\")\n",
    "# plt.ylabel(f\"{LowMassWIMPSource.mw} GeV/c^2 WIMP acceptance\")\n",
    "# plt.xscale('log')\n",
    "# plt.xlim(1e-5, 1e-1)\n",
    "# plt.ylim(0., 1)\n",
    "# plt.legend(loc='best')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# #plt.savefig('fd_roc_15GeV.png', dpi=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "colab_type": "code",
    "id": "79fwHbSs5oxX",
    "outputId": "9a67ef92-d454-46fa-97ca-c14e0181bce6"
   },
   "outputs": [],
   "source": [
    "# for rn, roc in rocs_nrbkg.items():\n",
    "#     x, y = roc['cdfs']['nr_bkg'], roc['cdfs']['nr']\n",
    "#     plt.errorbar(x, y,\n",
    "#                  xerr=np.abs(roc['cdf_intervals']['nr_bkg'] - x),\n",
    "#                  yerr=np.abs(roc['cdf_intervals']['nr'] - y),\n",
    "#                  linestyle='',\n",
    "#                  color=roc_colors[rn],\n",
    "#                  label=roc_labels[rn])\n",
    "# plt.plot(*dsets['nr']['roc_from_histogram_nrbkg'], \n",
    "#          label='2D Hist ($N = \\infty$)',\n",
    "#          color='k', linestyle='--')\n",
    "#      \n",
    "# plt.xlabel(\"NR radiogenics background\")\n",
    "# plt.ylabel(f\"{LowMassWIMPSource.mw} GeV/c^2 WIMP acceptance\")\n",
    "# plt.xscale('log')\n",
    "# plt.xlim(1e-5, 1)\n",
    "# plt.ylim(0., 1)\n",
    "# plt.legend(loc='best')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# #plt.savefig('fd_roc_nr_bkg_15GeV.png', dpi=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "vyKfsg0GbZKn",
    "outputId": "b2c587af-769a-4352-d915-4278df91e050"
   },
   "outputs": [],
   "source": [
    "# for rn, roc in rocs_mod.items():\n",
    "#     x, y = roc['cdfs']['nr'], roc['cdfs']['nr_mod']\n",
    "#     plt.errorbar(x, y,\n",
    "#                  xerr=np.abs(roc['cdf_intervals']['nr'] - x),\n",
    "#                  yerr=np.abs(roc['cdf_intervals']['nr_mod'] - y),\n",
    "#                  linestyle='',\n",
    "#                  #marker='.',\n",
    "#                  color=roc_colors[rn],\n",
    "#                  label=roc_labels[rn]\n",
    "#                 )\n",
    "# plt.plot(*dsets['nr']['roc_from_histogram_mod'], \n",
    "#          label='2D Hist ($N = \\infty$)',\n",
    "#          color='k', linestyle='--')\n",
    "# #plt.plot([0,1],[0,1], c='k', linewidth=1)\n",
    "# plt.xlabel(\"non mod wimp\")\n",
    "# plt.ylabel(f\"{LowMassWIMPSource.mw} GeV/c^2 WIMP acceptance\")\n",
    "# plt.xscale('log')\n",
    "# plt.xlim(1e-5, 1)\n",
    "# plt.ylim(0., 1)\n",
    "# plt.legend(loc='best')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# #plt.savefig('fd_roc_nr_bkg_15GeV.png', dpi=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined ROC curve plot\n",
    "plt.figure()\n",
    "\n",
    "diag = np.geomspace(1e-5, 1, 100)\n",
    "plt.plot(diag, diag, c='k', alpha=0.5)\n",
    "\n",
    "plt.plot(*dsets['nr']['roc_from_histogram_mod'], \n",
    "         #label='2D Hist ($N = \\infty$)',\n",
    "         color='C0', linestyle='--')\n",
    "\n",
    "roc = rocs_mod[('full', 'mod')]\n",
    "x, y = roc['cdfs']['nr'], roc['cdfs']['nr_mod']\n",
    "plt.errorbar(x, y,\n",
    "             xerr=np.abs(roc['cdf_intervals']['nr'] - x),\n",
    "             yerr=np.abs(roc['cdf_intervals']['nr_mod'] - y),\n",
    "             #linestyle='',\n",
    "             #marker='.',\n",
    "             color='C0', #roc_colors[rn],\n",
    "             label='WIMP-like background',\n",
    "            )\n",
    "\n",
    "plt.plot(*dsets['nr']['roc_from_histogram_nrbkg'], \n",
    "         #label='2D Hist ($N = \\infty$)',\n",
    "         color='C1', linestyle='--')\n",
    "roc = rocs_nrbkg[('full', 'nr_mod')]\n",
    "x, y = roc['cdfs']['nr_bkg'], roc['cdfs']['nr']\n",
    "plt.errorbar(x, y,\n",
    "             xerr=np.abs(roc['cdf_intervals']['nr_bkg'] - x),\n",
    "             yerr=np.abs(roc['cdf_intervals']['nr'] - y),\n",
    "             #linestyle='',\n",
    "             color='C1',\n",
    "             label='neutron background',\n",
    "            )\n",
    "\n",
    "plt.plot(*dsets['nr']['roc_from_histogram'], \n",
    "         #label='2D Hist ($N = \\infty$)',\n",
    "         color='C2', linestyle='--')\n",
    "\n",
    "roc = rocs[('full', 'nr_mod')]\n",
    "x, y = roc['cdfs']['er'], roc['cdfs']['nr']\n",
    "plt.errorbar(x, y,\n",
    "             xerr=np.abs(roc['cdf_intervals']['er'] - x),\n",
    "             yerr=np.abs(roc['cdf_intervals']['nr'] - y),\n",
    "             #linestyle='',\n",
    "             color='C2',\n",
    "             label='ER background',\n",
    "            )\n",
    "\n",
    "plt.xlabel(\"Background acceptance\")\n",
    "plt.ylabel(f\"{LowMassWIMPSource.mw} GeV/c$^2$ WIMP acceptance\")\n",
    "#plt.xscale('log')\n",
    "plt.xlim(1e-5, 1)\n",
    "plt.ylim(0., 1)\n",
    "plt.legend(loc='lower right', frameon=False)\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('fd_rocs_15GeV_326elife.png', dpi=200, bbox_to_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "sWWRgMb26VAd",
    "outputId": "d5039b1d-1aef-446e-9fb8-fbe0908ad4d4"
   },
   "outputs": [],
   "source": [
    "def bg_reduction_factor(nr_acceptance, er_leakage):\n",
    "    orig_roc = dsets['nr']['roc_from_histogram']\n",
    "    leakage_hist = np.interp(nr_acceptance, orig_roc[1], orig_roc[0])\n",
    "    return leakage_hist / er_leakage\n",
    "\n",
    "for rocname, roc in rocs.items():\n",
    "    acc, leak = roc['cdfs']['nr'], roc['cdfs']['er']   \n",
    "    red = bg_reduction_factor(acc, leak)\n",
    "    red_bounds = np.stack([\n",
    "        bg_reduction_factor(acc, q)\n",
    "        for q in roc['cdf_intervals']['er']])\n",
    "\n",
    "    plt.plot(acc, \n",
    "             red, \n",
    "             label=roc_labels[rocname], \n",
    "             color=roc_colors[rocname])\n",
    "    plt.fill_between(acc,\n",
    "                     red_bounds.min(axis=0), \n",
    "                     red_bounds.max(axis=0), \n",
    "                     color=roc_colors[rocname],\n",
    "                     alpha=0.3, linewidth=0, step='mid')  \n",
    "    \n",
    "    # If you're really paranoid, you might want to plot the xerror too:\n",
    "    # plt.errorbar(acc, \n",
    "    #              red, \n",
    "    #              xerr=np.abs(roc['cdf_intervals']['nr'] - acc),\n",
    "    #              yerr=[red - red_bounds.min(axis=0),\n",
    "    #                    red_bounds.max(axis=0) - red],\n",
    "    #              label=roclabels[rocname])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=0.2, c='k', linestyle='-')\n",
    "\n",
    "plt.xlabel(\"NR acceptance\")\n",
    "plt.ylabel(\"Background reduction factor\")\n",
    "\n",
    "plt.ylim(0, 4)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "#plt.ylim(0.8, 1.2)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('fd_bg_reduction_15GeV_326elife.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_W52iIcnonR"
   },
   "source": [
    "## Compare sensitivity\n",
    "\n",
    "Create likelihoods for:\n",
    "  1. ER vs NR histogram-based\n",
    "  2. ER vs NR flamedisx\n",
    "  3. ER vs NR-mod histogram-based -- could omit, should be same as (1)\n",
    "  4. ER vs NR-mod flamedisx\n",
    "\n",
    "Sources are represented by `ColumnSource`s, with a simulate method that draws from `dsets[dname]['data']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "uOKdgmTfnqWd",
    "outputId": "182fb669-6360-4be0-ba5f-a66d43509bf2"
   },
   "outputs": [],
   "source": [
    "class FastSource(fd.ColumnSource):\n",
    "\n",
    "    def _differential_rate(self, data_tensor, ptensor):\n",
    "        return tf.clip_by_value(self._fetch(self.column, data_tensor) * self.scale_by,\n",
    "                                1e-20, 1e20)\n",
    "\n",
    "    def random_truth(self, n_events, fix_truth=None, **params):\n",
    "        if fix_truth is not None or len(params):\n",
    "            raise NotImplementedError\n",
    "        return dsets[self.dname]['data'].sample(n_events, replace=True)\n",
    "\n",
    "mu_of_source = dict(er=1, nr=1, nr_mod=1, nr_bkg=1)\n",
    "likelihood_types = ('mh', 'full')\n",
    "\n",
    "fast_sources = {\n",
    "    ltype: {\n",
    "        dname: type(\n",
    "            f'FS_{dname}_{ltype}', \n",
    "            (FastSource,), \n",
    "            dict(column=f'l_{ltype}_{dname}',\n",
    "                 dname=dname,\n",
    "                 # Adjust mu to the desired value, scaling\n",
    "                 # the differential rate accordingly\n",
    "                 mu=mu_of_source[dname],\n",
    "                 scale_by=mu_of_source[dname]/dsets[dname]['events_per_bin'].n,\n",
    "                 ))\n",
    "        for dname in dsets.keys()}\n",
    "    for ltype in likelihood_types}\n",
    "\n",
    "likelihoods = {\n",
    "    ltype: {\n",
    "        nrc: fd.LogLikelihood(\n",
    "            sources=dict(er=fast_sources[ltype]['er'],\n",
    "                         nr=fast_sources[ltype][nrc],\n",
    "                         nr_bkg=fast_sources[ltype]['nr_bkg'],\n",
    "                         ),\n",
    "            free_rates=('er', 'nr', 'nr_bkg'),\n",
    "            # max_sigma and n_trials are irrelevant for ColumnSource\n",
    "            batch_size=10_000)\n",
    "        for nrc in nr_conditions}\n",
    "    for ltype in likelihood_types}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMOsgSzNCH5S"
   },
   "outputs": [],
   "source": [
    "# # Check simulated events are reasonable\n",
    "# ll = likelihoods['full']['nr']\n",
    "# for sname, color in [['er', 'b'], ['nr', 'r']]:\n",
    "#     d = ll.sources[sname].simulate(1000)\n",
    "#     plt.scatter(d['cs1'], d['cs2'], color=color, s=1)\n",
    "# likelihoods['full']['nr']\n",
    "# d = ll.simulate()\n",
    "# plt.scatter(d['cs1'], d['cs2'], s=1, c='g')\n",
    "# from collections import Counter\n",
    "# Counter(d['source'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqKz2M1TGNF6"
   },
   "source": [
    "Time for 400 trials, 1e4 ER events /trials, minuit, default tolerance:\n",
    "  * master 23 January, colab GPU: 41 minutes\n",
    "  * trace_likelihood 25 January, Jelle's laptop: 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.inference.LOWER_RATE_MULTIPLIER_BOUND = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nrs = np.linspace(1e-9, 24, 40)\n",
    "#ers = np.linspace(6100, 6500, 40)\n",
    "#xx, yy = np.meshgrid(nrs, ers)\n",
    "\n",
    "n_trials = 10000\n",
    "allow_failure = False\n",
    "\n",
    "rm_scale = 15.  # Exposure scale [tonne year]\n",
    "er_bkg_reduction = 8.2  # ER background reduction from Xe1T to XenT\n",
    "nr_bkg_reduction = 40  # NR background reduction from Xe1T to XenT\n",
    "discovery=False\n",
    "\n",
    "xtol=1e-4\n",
    "gtol=1e-3\n",
    "\n",
    "n_bestfit_failures = 0\n",
    "n_cond_bestfit_failures = 0\n",
    "n_guess_limit_failures = 0\n",
    "n_limit_failures = 0\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sensi_results = {\n",
    "    ltype: {\n",
    "        nrc: pd.DataFrame(\n",
    "            dict(t=np.zeros(n_trials),\n",
    "                 bestfit_nr=np.zeros(n_trials),\n",
    "                 bestfit_er=np.zeros(n_trials),\n",
    "                 condfit_er=np.zeros(n_trials),\n",
    "                 limit=np.zeros(n_trials))).to_records()\n",
    "        for nrc in nr_conditions}\n",
    "    for ltype in likelihood_types}\n",
    "\n",
    "#(er=627, nr=1, nr_mod=1, nr_bkg=1.4)\n",
    "truth = dict(nr_rate_multiplier=0.,\n",
    "             er_rate_multiplier=627. * rm_scale / er_bkg_reduction,\n",
    "             nr_bkg_rate_multiplier=1.4 * rm_scale / nr_bkg_reduction)\n",
    "\n",
    "# Expected total number of wimps at reference xsec of 1e-45\n",
    "nwimps = 627 * rm_scale * dsets['nr_mod']['events_per_bin'].n / dsets['er']['events_per_bin'].n\n",
    "if discovery:\n",
    "    nwimps *= 2e-47 / 1e-45  # desired xsec / wimprates xsec\n",
    "    truth['nr_rate_multiplier'] = nwimps\n",
    "\n",
    "import warnings\n",
    "\n",
    "for trial_i in tqdm(range(n_trials)):\n",
    "    # The nr model choice changes what events we need to pick from,\n",
    "    # so this has to be the outer loop.\n",
    "    for nrc in nr_conditions: \n",
    "\n",
    "        # Draw simulated events.\n",
    "        # The histogram and flamedisx likelihood draw from the same\n",
    "        # event reservoir (they just look at different columns later)\n",
    "        d = likelihoods['mh'][nrc].simulate(**truth)\n",
    "\n",
    "        for ltype in likelihood_types:\n",
    "            #if discovery:  # Only use mh nr and full nrmod, maybe also for sensi.. yeah\n",
    "            if f'{ltype}_{nrc}' not in ['mh_nr', 'full_nr_mod']:\n",
    "                continue\n",
    "            \n",
    "            q = sensi_results[ltype][nrc]\n",
    "\n",
    "            ll = likelihoods[ltype][nrc]\n",
    "            ll.set_data(d)\n",
    "\n",
    "            guess = dict(nr_rate_multiplier=3, \n",
    "                         er_rate_multiplier=len(d))\n",
    "            \n",
    "            try:\n",
    "                bf = ll.bestfit(\n",
    "                    guess=guess,\n",
    "                    fix=dict(nr_bkg_rate_multiplier=truth['nr_bkg_rate_multiplier']),\n",
    "                    optimizer_kwargs=dict(options=dict(gtol=gtol, xtol=xtol)),\n",
    "                    allow_failure=allow_failure,\n",
    "                )\n",
    "            except:\n",
    "                n_bestfit_failures += 1\n",
    "                print(\"bestfit failure\")\n",
    "                continue\n",
    "            q['bestfit_nr'][trial_i] = bf['nr_rate_multiplier']\n",
    "            q['bestfit_er'][trial_i] = bf['er_rate_multiplier']\n",
    "            \n",
    "            #continue\n",
    "            \n",
    "            # lls = [-2*ll(nr_rate_multiplier=x,\n",
    "            #              er_rate_multiplier=y,\n",
    "            #              nr_bkg_rate_multiplier=truth['nr_bkg_rate_multiplier'])\n",
    "            #        for (x, y) in np.array([xx.ravel(), yy.ravel()]).T]\n",
    "            # ll_min = min(lls)\n",
    "            # lls = np.reshape(lls, (40, 40))\n",
    "            # \n",
    "            # grid_min = np.unravel_index(np.argmin(lls), lls.shape)\n",
    "            # \n",
    "            # plt.figure(figsize=(10, 8))\n",
    "            # plt.pcolormesh(xx, yy, (lls - ll_min), vmin=0, vmax=16)\n",
    "            # plt.colorbar()\n",
    "            # plt.plot([3, 0], [truth['er_rate_multiplier'],\n",
    "            #                   truth['er_rate_multiplier']], 'o', color='orange')\n",
    "            # plt.plot(nrs[grid_min[1]], ers[grid_min[0]], 'go')\n",
    "            # plt.plot(bf['nr_rate_multiplier'], bf['er_rate_multiplier'], 'ro')\n",
    "            # plt.title(f'{ltype} {nrc}')\n",
    "            # plt.show()\n",
    "            # \n",
    "            # continue\n",
    "\n",
    "            # Compute best fit nuisance params conditional on truth \n",
    "            # and test statistic value (which depends on this)\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    cf = ll.bestfit(\n",
    "                        guess=dict(er_rate_multiplier=bf['er_rate_multiplier']),\n",
    "                        fix=dict(nr_rate_multiplier=0 if discovery else truth['nr_rate_multiplier'],\n",
    "                                 nr_bkg_rate_multiplier=truth['nr_bkg_rate_multiplier']),\n",
    "                        optimizer_kwargs=dict(options=dict(gtol=gtol, xtol=xtol)),\n",
    "                        allow_failure=allow_failure)\n",
    "            except:\n",
    "                n_cond_bestfit_failures += 1\n",
    "                print(\"conditional bestfit failure\")\n",
    "            q['condfit_er'][trial_i] = cf['er_rate_multiplier']\n",
    "            q['t'][trial_i] = 2 * (ll(**bf) - ll(**cf))\n",
    "            \n",
    "            if discovery:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                guess_limit = ll.limit(\n",
    "                    'nr_rate_multiplier', \n",
    "                    bestfit=bf,\n",
    "                    fix={k: v for k, v in bf.items()\n",
    "                         if k != 'nr_rate_multiplier'},\n",
    "                    optimizer_kwargs=dict(options=dict(gtol=gtol, xtol=xtol)),\n",
    "                    allow_failure=allow_failure)\n",
    "            except:\n",
    "                n_guess_limit_failures += 1\n",
    "                print(\"guess limit failure\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                q['limit'][trial_i] = ll.limit(\n",
    "                    'nr_rate_multiplier', \n",
    "                    bestfit=bf,\n",
    "                    guess={**bf, **dict(nr_rate_multiplier=guess_limit)},\n",
    "                    fix=dict(nr_bkg_rate_multiplier=truth['nr_bkg_rate_multiplier']),\n",
    "                    optimizer_kwargs=dict(options=dict(gtol=gtol, xtol=xtol)),\n",
    "                    allow_failure=False)\n",
    "            except:\n",
    "                n_limit_failures += 1\n",
    "                print(\"limit failure\")\n",
    "                \n",
    "            continue\n",
    "            \n",
    "            lls = [-2*ll(nr_rate_multiplier=x,\n",
    "                         er_rate_multiplier=y,\n",
    "                         nr_bkg_rate_multiplier=truth['nr_bkg_rate_multiplier'])\n",
    "                   for (x, y) in np.array([xx.ravel(), yy.ravel()]).T]\n",
    "            m2ll_best = -2*ll(**bf)\n",
    "            wilks_crit = stats.norm.ppf(0.9) ** 2\n",
    "            \n",
    "            fun = (np.array(lls) - (m2ll_best + wilks_crit))**2\n",
    "            fun_min = min(fun)\n",
    "            fun = np.reshape(fun, (40, 40))\n",
    "            \n",
    "            fun -= xx * 0.01\n",
    "            \n",
    "            grid_min = np.unravel_index(np.argmin(fun), fun.shape)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.pcolormesh(xx, yy, (fun - fun_min), vmin=0, vmax=9)\n",
    "            plt.colorbar()\n",
    "            plt.plot([3, 0], [truth['er_rate_multiplier'],\n",
    "                              truth['er_rate_multiplier']], 'o', color='orange', label='truth and guess')\n",
    "            plt.plot(nrs[grid_min[1]], ers[grid_min[0]], 'go', label='grid minimum')\n",
    "            plt.plot(bf['nr_rate_multiplier'], bf['er_rate_multiplier'], 'ro', label='bestfit')\n",
    "            plt.axvline(q['limit'][trial_i], color='r', linestyle='--', label='limit')\n",
    "            plt.legend()\n",
    "            plt.title(f'{ltype} {nrc}')\n",
    "            plt.show()\n",
    "\n",
    "print(\"n bestfit failures\", n_bestfit_failures)\n",
    "print(\"n conditional bestfit failures\", n_cond_bestfit_failures)\n",
    "print(\"n guess limit failures: \", n_guess_limit_failures)\n",
    "print(\"n limit failures: \", n_limit_failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fn = 'disc_results_20200219_10k_15ty_200GeV_2e-47.pkl'\n",
    "#fn = 'sensi_results_20200219_10k_15ty_200GeV.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, mode='wb') as f:\n",
    "    pickle.dump(sensi_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, mode='rb') as f:\n",
    "    sensi_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sensi files for different exposures\n",
    "exposures = [5, 10, 15, 20]  #, 25]  # tonne year\n",
    "disc_sigmas = {k: dict() for k in exposures}\n",
    "for exp in exposures:\n",
    "    #name = f'disc_results_20200218_1k_{exp}ty_200GeV_2e-47.pkl'  # [5, 10, 15, 20]\n",
    "    name = f'disc_results_20200219_10k_{exp}ty_200GeV_2e-47.pkl'  # [5, 10, 15, 20]\n",
    "    with open(name, mode='rb') as f:\n",
    "        disc_res = pickle.load(f)\n",
    "\n",
    "    for (ltype, nrc) in [('mh', 'nr'), ('full', 'nr_mod')]:            \n",
    "        q_t = disc_res[ltype][nrc]['t']\n",
    "        q_t = q_t[np.isfinite(q_t) & (q_t >= 0)]\n",
    "        sigmas = q_t**0.5\n",
    "        \n",
    "        percentiles = np.percentile(sigmas, stats.norm.cdf([-2, -1, 0, 1, 2])*100)\n",
    "        disc_sigmas[exp][ltype] = percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.25\n",
    "label_set = False\n",
    "plt.figure()\n",
    "for i in [3, 4, 5]:\n",
    "    plt.axhline(i, color='gray', linestyle='--')\n",
    "for exp in exposures:\n",
    "    for (ltype, offset) in [('mh', -1.5*delta), ('full', 1.5*delta)]:\n",
    "        p = disc_sigmas[exp][ltype]\n",
    "        c, l = ('C0', '2D Hist (sampled)') if ltype == 'mh' else ('orange', 'Flamedisx')\n",
    "        plt.fill_between([exp-delta+offset, exp+delta+offset], 2*[p[4]], 2*[p[0]],\n",
    "                         color=c, alpha=0.4, lw=0)\n",
    "        plt.fill_between([exp-delta+offset, exp+delta+offset], 2*[p[3]], 2*[p[1]],\n",
    "                         color=c, alpha=1, lw=0, label=l if not label_set else None)\n",
    "        plt.plot([exp-delta+offset, exp+delta+offset], 2*[p[2]], color='k')\n",
    "    label_set = True\n",
    "\n",
    "def s_to_p(s):\n",
    "    return stats.chi2(1).sf(s**2)/2    \n",
    "    \n",
    "def p_to_s(p):\n",
    "    return -stats.norm.ppf(p)\n",
    "    \n",
    "plt.xlim(0, max(exposures)+5)\n",
    "plt.ylim(0, 9)\n",
    "plt.xlabel('Exposure [tonne year]')\n",
    "plt.ylabel('Discovery significance [sigma]')\n",
    "plt.legend(frameon=False, loc='upper left')\n",
    "\n",
    "#secax = plt.gca().secondary_yaxis('right', functions=(s_to_p, p_to_s))\n",
    "#print(secax.get_yticks())\n",
    "secax = plt.gca().twinx()\n",
    "secax.set_ylabel('Discovery p-value')\n",
    "secax.set_ylim(0, 9)\n",
    "\n",
    "secax.set_yticklabels(['%.2g' % s_to_p(i) for i in range(10)])\n",
    "\n",
    "#plt.savefig('discovery_exposure_sig_10k.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 1, figsize=(7, 10))\n",
    "\n",
    "def hist_plot(data, bins, color, label):\n",
    "    data = data[np.isfinite(data)]\n",
    "    mean = np.mean(data)\n",
    "    # Error calculation is very dodgy, since dist\n",
    "    # is far from Gaussian.\n",
    "    err = np.std(data)/len(data)**0.5\n",
    "\n",
    "    Hist1d(data, bins=bins).plot(\n",
    "        color=color, \n",
    "        set_xlim=True,\n",
    "        errors=True,\n",
    "        error_style='band',\n",
    "        label=f'{label}: {mean:.2f} $\\pm$ {err:.2f}')\n",
    "    plt.axvline(mean,\n",
    "                color=color,\n",
    "                linestyle='--')\n",
    "    plt.legend(loc='best', frameon=False)\n",
    "\n",
    "def norm_plot(data, bins, color='k', label=r'$\\mathcal{N}(\\mu,\\sigma)$', alpha=0.6):\n",
    "    bin_centers = (bins[:-1] + bins[1:])/2\n",
    "    plt.plot(bin_centers,\n",
    "             n_trials*np.diff(stats.norm.cdf(bins,\n",
    "                                             loc=np.nanmean(data),\n",
    "                                             scale=np.nanstd(data))),\n",
    "             color=color, alpha=alpha, label=label)\n",
    "\n",
    "n_bins = 50\n",
    "pvals = False\n",
    "x_max = 70 if discovery else 5\n",
    "x_min = 0 if discovery else -0.2\n",
    "\n",
    "done=False\n",
    "\n",
    "for ltype in likelihood_types:\n",
    "    for nrc in nr_conditions: \n",
    "        if nrc == 'nr' and ltype == 'full': continue\n",
    "        if (ltype, nrc) not in rocs:\n",
    "            continue\n",
    "\n",
    "        color = roc_colors[(ltype, nrc)]\n",
    "        label = roc_labels[(ltype, nrc)]\n",
    "        if label.startswith('Fd'):\n",
    "            label = 'Flamedisx'\n",
    "                \n",
    "        q = sensi_results[ltype][nrc]\n",
    "\n",
    "        ####################################################################################\n",
    "        plt.sca(axes[0])\n",
    "        bins = np.linspace(0, x_max, n_bins)\n",
    "        hist_plot(q['bestfit_nr'],\n",
    "                  bins=bins,\n",
    "                  label=label, color=color)\n",
    "        plt.xlabel(\"Best fit [DM events]\")\n",
    "        if discovery:\n",
    "            plt.ylim(0, n_trials/11)\n",
    "            norm_plot(q['bestfit_nr'], bins)\n",
    "        else:\n",
    "            plt.yscale('log')\n",
    "            plt.ylim(0.7, n_trials)\n",
    "\n",
    "        #####################################################################################\n",
    "        plt.sca(axes[1])\n",
    "        bins = np.linspace(x_min, x_max, n_bins)\n",
    "        hist_plot(q['t'],\n",
    "                  bins=bins,\n",
    "                  label=label, color=color)\n",
    "        plt.xlabel(r'$-2\\ln\\lambda(0)$') if discovery else plt.xlabel(r'$-2\\ln\\lambda(\\mu)$')\n",
    "\n",
    "        # plot asymptotic dist\n",
    "        if discovery:\n",
    "            plt.ylim(0, n_trials/14)\n",
    "            bin_centers = (bins[:-1] + bins[1:])/2\n",
    "     \n",
    "            p_vals = stats.chi2(1).sf(q['t'])/2\n",
    "            #sigmas = -stats.norm.ppf(p_vals)\n",
    "            # Alternatively Z = sqrt(q0)\n",
    "            sigmas = q['t']**0.5\n",
    "            \n",
    "            # plot noncentral chi2\n",
    "            # BOTH of these should work\n",
    "            #non_centrality = (0 - np.mean(q['bestfit_nr']))**2 / np.std(q['bestfit_nr'])**2\n",
    "            non_centrality = np.median(q['t'])  # from Asimov likelihood\n",
    "\n",
    "            plt.plot(bin_centers,\n",
    "                     n_trials*np.diff(stats.ncx2.cdf(x=bins, df=1, nc=non_centrality)),\n",
    "                    'k', alpha=0.6, label=r'$\\chi^{2}(k=1, \\lambda=$' + 'median' + r'$[q_{0}])$')\n",
    "\n",
    "            # from Walk approx (checked this is equal to stats.ncx2 with non centr mu and sigma)\n",
    "            #def f(q0, mu, sigma):\n",
    "            #    # + (1 - Phi(mu/sigma))delta(q0)\n",
    "            #    return np.exp(-1*(q0**0.5 - (mu/sigma))**2/2) / (2 * (2*np.pi)**0.5 * q0**0.5)\n",
    "\n",
    "            #plt.plot(bin_centers,\n",
    "            #         n_trials * (50.2/n_bins) * f(bin_centers, np.mean(q['bestfit_nr']), np.std(q['bestfit_nr'])),\n",
    "            #         'r')\n",
    "        else:\n",
    "            plt.yscale('log')\n",
    "            plt.ylim(0.7, n_trials)\n",
    "\n",
    "            bins = np.linspace(x_min, x_max, n_bins)\n",
    "            bin_centers = (bins[:-1] + bins[1:])/2\n",
    "            #plt.plot(bin_centers, n_trials*0.5*np.diff(stats.chi2(1).cdf(bins)), 'k',\n",
    "            #         label=r'$\\frac{1}{2}\\chi^{2}(k=1)$')\n",
    "            # high res chi2\n",
    "            test_ax = np.linspace(x_min, x_max, 200)\n",
    "            plt.plot(test_ax, n_trials*0.5*stats.chi2(1).pdf(test_ax)*(bins[1]-bins[0]), 'k',\n",
    "                     label=r'$\\frac{1}{2}\\chi^{2}(k=1)$')\n",
    "\n",
    "        ##########################################################################################\n",
    "        plt.sca(axes[2])\n",
    "        if discovery:\n",
    "            if pvals:\n",
    "                bins = np.geomspace(1e-7, 1, n_bins)\n",
    "                hist_plot(p_vals,\n",
    "                          bins=bins,\n",
    "                          label=label, color=color)\n",
    "                plt.xscale('log')\n",
    "                plt.xlabel('Discovery p-value')\n",
    "            else:\n",
    "                bins = np.linspace(0, 8, n_bins)\n",
    "                hist_plot(sigmas,\n",
    "                          bins=bins,\n",
    "                          label=label, color=color)\n",
    "                plt.xlabel('Discovery sigma')\n",
    "                \n",
    "                norm_plot(sigmas, bins)            \n",
    "            plt.ylim(0, None)\n",
    "        else:\n",
    "            bins = np.geomspace(0.5, 100, n_bins)\n",
    "            hist_plot(q['limit'], \n",
    "                      bins=bins,\n",
    "                      label=label, color=color)\n",
    "            plt.xlabel(\"Limit [DM events]\")\n",
    "            plt.xscale('log')\n",
    "            plt.ylim(0, max(10, n_trials / 11))\n",
    "            plt.xlim(0.5, 100)\n",
    "            \n",
    "            if not done:\n",
    "                done=True\n",
    "                \n",
    "                def n_to_x(n):\n",
    "                    return n * 1e-45 / nwimps\n",
    "                def x_to_n(x):\n",
    "                    return x * nwimps / 1e-45\n",
    "                # xsec scale? compute expected n wimps, scale to where 1 wimp is 1e-45\n",
    "                #nwimps\n",
    "                secax = plt.gca().secondary_xaxis('top', functions=(n_to_x, x_to_n))\n",
    "                #secax = plt.gca().twiny()\n",
    "                secax.set_xlabel(r'Limit [cm$^2$]')\n",
    "                #secax.set_xscale('log')\n",
    "                #secax.set_xlim(0.5, 100)\n",
    "\n",
    "                #new_tick_vals = np.geomspace(1e-48, 1e-47, 2) #secax.get_xticks()\n",
    "                #tick_vals = new_tick_vals * nwimps / 1e-45\n",
    "                #print(tick_vals)\n",
    "                \n",
    "                #secax.set_xticks(tick_vals)\n",
    "                #secax.set_xticklabels(['%.2g' % (t*1e-45/nwimps) for t in tick_vals])\n",
    "                #secax.set_xticklabels(['%.2g' % t for t in new_tick_vals])\n",
    "\n",
    "for i_ax, ax in enumerate(axes):\n",
    "    ax.set_ylabel(\"Trials / bin\")\n",
    "\n",
    "    if discovery or i_ax == 1: \n",
    "        plt.sca(ax)\n",
    "        # Sort legend entries\n",
    "        # https://stackoverflow.com/a/46160465\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        order = [0,2,1]\n",
    "        plt.legend([handles[idx] for idx in order],\n",
    "                   [labels[idx] for idx in order],\n",
    "                   frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "#plt.savefig('discovery_5ty_200GeV_10k.png', dpi=200, bbox_to_inches='tight')\n",
    "#plt.savefig('sensitivity_15ty_200GeV_10k.png', dpi=200, bbox_to_inches='tight')\n",
    "\n",
    "\n",
    "#plt.savefig('sensitivity_results_10ty_30wimps_200GeV_20200217_disc.png', dpi=200, bbox_inches='tight')\n",
    "# Bottom panel for the paper\n",
    "#extent = axes[2].get_window_extent().transformed(f.dpi_scale_trans.inverted())\n",
    "#f.savefig('discovery_power.png', dpi=200, bbox_inches=extent.expanded(1.2, 1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0, 1, 10_000)\n",
    "\n",
    "log=False\n",
    "mw = 200\n",
    "exposures = [10, 15, 20]  # tonne year\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "for i, exp in enumerate(exposures):\n",
    "    name = f'sensi_results_20200219_10k_{exp}ty_{mw}GeV.pkl'\n",
    "    #name = 'sensi_results_20200225_1k_15ty_200GeV_xtol-6_gtol-5.pkl'\n",
    "    with open(name, mode='rb') as f:\n",
    "        sensi_res = pickle.load(f)\n",
    "    r = sensi_res['full']['nr_mod']['t']\n",
    "    r_mh = sensi_res['mh']['nr']['t']\n",
    "\n",
    "    plt.plot(1 - (0.5 + 0.5*stats.chi2(1).cdf(np.sort(r).clip(0, None))),\n",
    "             1-a,\n",
    "             color=f'C{i}', label=f'{exp}ty')\n",
    "    plt.plot(1 - (0.5 + 0.5*stats.chi2(1).cdf(np.sort(r_mh).clip(0, None))),\n",
    "             1-a,\n",
    "             color=f'C{i}', linestyle='--')  # label=f'{exp}ty')\n",
    "plt.legend(frameon=False)\n",
    "min_p = 1e-3\n",
    "max_p = 1 if log else 0.55\n",
    "plt.plot([min_p, max_p], [min_p, max_p], 'k--')\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.fill_between([0.5, 1], 2*[1], 2*[min_p], color='lightgray', lw=0)\n",
    "plt.gca().set_aspect(1)\n",
    "if log:\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "plt.ylim(min_p, max_p)\n",
    "plt.xlim(min_p, max_p)\n",
    "plt.xlabel('Asymptotic p-value')\n",
    "plt.ylabel('Observed p-value')\n",
    "#plt.savefig('pvalue_check_200GeV_10k_linlin.png', dpi=200, bbox_to_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ernr_disc_sens.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
